{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진분류 평가하기\n",
    "\n",
    "import pandas as pd\n",
    "y_true = pd.DataFrame([1, 1, 1, 0, 0, 1, 1, 1, 1, 0]) #실제값\n",
    "y_pred = pd.DataFrame([1, 0, 1, 1, 0, 0, 0, 1, 1, 0]) #예측값\n",
    "\n",
    "y_true_str = pd.DataFrame(['A', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'B']) #실제값\n",
    "y_pred_str = pd.DataFrame(['A', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'B']) #예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _classification\n",
      "    _dist_metrics\n",
      "    _pairwise_distances_reduction (package)\n",
      "    _pairwise_fast\n",
      "    _plot (package)\n",
      "    _ranking\n",
      "    _regression\n",
      "    _scorer\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.metrics._dist_metrics.DistanceMetric\n",
      "        sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
      "        sklearn.metrics._plot.regression.PredictionErrorDisplay\n",
      "    sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin(builtins.object)\n",
      "        sklearn.metrics._plot.det_curve.DetCurveDisplay\n",
      "        sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay\n",
      "        sklearn.metrics._plot.roc_curve.RocCurveDisplay\n",
      "    \n",
      "    class ConfusionMatrixDisplay(builtins.object)\n",
      "     |  ConfusionMatrixDisplay(confusion_matrix, *, display_labels=None)\n",
      "     |  \n",
      "     |  Confusion Matrix visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_predictions` to\n",
      "     |  create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n",
      "     |  attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  confusion_matrix : ndarray of shape (n_classes, n_classes)\n",
      "     |      Confusion matrix.\n",
      "     |  \n",
      "     |  display_labels : ndarray of shape (n_classes,), default=None\n",
      "     |      Display labels for plot. If None, display labels are set from 0 to\n",
      "     |      `n_classes - 1`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  im_ : matplotlib AxesImage\n",
      "     |      Image representing the confusion matrix.\n",
      "     |  \n",
      "     |  text_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n",
      "     |      Array of matplotlib axes. `None` if `include_values` is false.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with confusion matrix.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the confusion matrix.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "     |      classification.\n",
      "     |  ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |      given an estimator, the data, and the label.\n",
      "     |  ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |      given the true and predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
      "     |  >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "     |  ...                               display_labels=clf.classes_)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, confusion_matrix, *, display_labels=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, *, include_values=True, cmap='viridis', xticks_rotation='horizontal', values_format=None, ax=None, colorbar=True, im_kw=None, text_kw=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`,\n",
      "     |          the format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |          Returns a :class:`~sklearn.metrics.ConfusionMatrixDisplay` instance\n",
      "     |          that contains all the information to plot the confusion matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True, im_kw=None, text_kw=None) from builtins.type\n",
      "     |      Plot Confusion Matrix given an estimator and some data.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |          given the true and predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> ConfusionMatrixDisplay.from_estimator(\n",
      "     |      ...     clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True, im_kw=None, text_kw=None) from builtins.type\n",
      "     |      Plot Confusion Matrix given true and predicted labels.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          The predicted labels given by the method `predict` of an\n",
      "     |          classifier.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |          given an estimator, the data, and the label.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> y_pred = clf.predict(X_test)\n",
      "     |      >>> ConfusionMatrixDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DetCurveDisplay(sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin)\n",
      "     |  DetCurveDisplay(*, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  DET curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`\n",
      "     |  or :func:`~sklearn.metrics.DetCurveDisplay.from_predictions` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  fnr : ndarray\n",
      "     |      False negative rate.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The label of the positive class.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      DET Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with DET Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  det_curve : Compute error rates for different probability thresholds.\n",
      "     |  DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |      some data.\n",
      "     |  DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |      predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import det_curve, DetCurveDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, test_size=0.4, random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |  >>> y_pred = clf.decision_function(X_test)\n",
      "     |  >>> fpr, fnr, _ = det_curve(y_test, y_pred)\n",
      "     |  >>> display = DetCurveDisplay(\n",
      "     |  ...     fpr=fpr, fnr=fnr, estimator_name=\"SVC\"\n",
      "     |  ... )\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DetCurveDisplay\n",
      "     |      sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          it is not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, response_method='auto', pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot DET curve given an estimator and data.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the predicted target response. If set\n",
      "     |          to 'auto', :term:`predict_proba` is tried first and if it does not\n",
      "     |          exist :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |          predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> DetCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, pos_label=None, name=None, ax=None, **kwargs) from builtins.type\n",
      "     |      Plot the DET curve given the true and predicted labels.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by `decision_function` on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |          some data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> DetCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DistanceMetric(builtins.object)\n",
      "     |  Uniform interface for fast distance metric functions.\n",
      "     |  \n",
      "     |  The `DistanceMetric` class provides a convenient way to compute pairwise distances\n",
      "     |  between samples. It supports various distance metrics, such as Euclidean distance,\n",
      "     |  Manhattan distance, and more.\n",
      "     |  \n",
      "     |  The `pairwise` method can be used to compute pairwise distances between samples in\n",
      "     |  the input arrays. It returns a distance matrix representing the distances between\n",
      "     |  all pairs of samples.\n",
      "     |  \n",
      "     |  The :meth:`get_metric` method allows you to retrieve a specific metric using its\n",
      "     |  string identifier.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.metrics import DistanceMetric\n",
      "     |  >>> dist = DistanceMetric.get_metric('euclidean')\n",
      "     |  >>> X = [[1, 2], [3, 4], [5, 6]]\n",
      "     |  >>> Y = [[7, 8], [9, 10]]\n",
      "     |  >>> dist.pairwise(X,Y)\n",
      "     |  array([[7.81..., 10.63...]\n",
      "     |         [5.65...,  8.48...]\n",
      "     |         [1.41...,  4.24...]])\n",
      "     |  \n",
      "     |  Available Metrics\n",
      "     |  \n",
      "     |  The following lists the string metric identifiers and the associated\n",
      "     |  distance metric classes:\n",
      "     |  \n",
      "     |  **Metrics intended for real-valued vector spaces:**\n",
      "     |  \n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  identifier      class name            args      distance function\n",
      "     |  --------------  --------------------  --------  -------------------------------\n",
      "     |  \"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
      "     |  \"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
      "     |  \"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
      "     |  \"minkowski\"     MinkowskiDistance     p, w      ``sum(w * |x - y|^p)^(1/p)``\n",
      "     |  \"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
      "     |  \"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  \n",
      "     |  **Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
      "     |  distance metric requires data in the form of [latitude, longitude] and both\n",
      "     |  inputs and outputs are in units of radians.\n",
      "     |  \n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  identifier    class name          distance function\n",
      "     |  ------------  ------------------  ---------------------------------------------------------------\n",
      "     |  \"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  \n",
      "     |  \n",
      "     |  **Metrics intended for integer-valued vector spaces:**  Though intended\n",
      "     |  for integer-valued vectors, these are also valid metrics in the case of\n",
      "     |  real-valued vectors.\n",
      "     |  \n",
      "     |  =============  ====================  ========================================\n",
      "     |  identifier     class name            distance function\n",
      "     |  -------------  --------------------  ----------------------------------------\n",
      "     |  \"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
      "     |  \"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
      "     |  \"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
      "     |  =============  ====================  ========================================\n",
      "     |  \n",
      "     |  **Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
      "     |  is evaluated to \"True\".  In the listings below, the following\n",
      "     |  abbreviations are used:\n",
      "     |  \n",
      "     |   - N  : number of dimensions\n",
      "     |   - NTT : number of dims in which both values are True\n",
      "     |   - NTF : number of dims in which the first value is True, second is False\n",
      "     |   - NFT : number of dims in which the first value is False, second is True\n",
      "     |   - NFF : number of dims in which both values are False\n",
      "     |   - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
      "     |   - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
      "     |  \n",
      "     |  =================  =======================  ===============================\n",
      "     |  identifier         class name               distance function\n",
      "     |  -----------------  -----------------------  -------------------------------\n",
      "     |  \"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
      "     |  \"matching\"         MatchingDistance         NNEQ / N\n",
      "     |  \"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
      "     |  \"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
      "     |  \"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
      "     |  \"russellrao\"       RussellRaoDistance       (N - NTT) / N\n",
      "     |  \"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
      "     |  \"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
      "     |  =================  =======================  ===============================\n",
      "     |  \n",
      "     |  **User-defined distance:**\n",
      "     |  \n",
      "     |  ===========    ===============    =======\n",
      "     |  identifier     class name         args\n",
      "     |  -----------    ---------------    -------\n",
      "     |  \"pyfunc\"       PyFuncDistance     func\n",
      "     |  ===========    ===============    =======\n",
      "     |  \n",
      "     |  Here ``func`` is a function which takes two one-dimensional numpy\n",
      "     |  arrays, and returns a distance.  Note that in order to be used within\n",
      "     |  the BallTree, the distance must be a true metric:\n",
      "     |  i.e. it must satisfy the following properties\n",
      "     |  \n",
      "     |  1) Non-negativity: d(x, y) >= 0\n",
      "     |  2) Identity: d(x, y) = 0 if and only if x == y\n",
      "     |  3) Symmetry: d(x, y) = d(y, x)\n",
      "     |  4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
      "     |  \n",
      "     |  Because of the Python object overhead involved in calling the python\n",
      "     |  function, this will be fairly slow, but it will have the same\n",
      "     |  scaling as other distances.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__ = __reduce_cython__(...)\n",
      "     |  \n",
      "     |  __reduce_cython__(self)\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  __setstate_cython__(self, __pyx_state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get_metric(metric, dtype=<class 'numpy.float64'>, **kwargs) from builtins.type\n",
      "     |      Get the given distance metric from the string identifier.\n",
      "     |      \n",
      "     |      See the docstring of DistanceMetric for a list of available metrics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      metric : str or class name\n",
      "     |          The string identifier or class name of the desired distance metric.\n",
      "     |          See the documentation of the `DistanceMetric` class for a list of\n",
      "     |          available metrics.\n",
      "     |      \n",
      "     |      dtype : {np.float32, np.float64}, default=np.float64\n",
      "     |          The data type of the input on which the metric will be applied.\n",
      "     |          This affects the precision of the computed distances.\n",
      "     |          By default, it is set to `np.float64`.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Additional keyword arguments that will be passed to the requested metric.\n",
      "     |          These arguments can be used to customize the behavior of the specific\n",
      "     |          metric.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      metric_obj : instance of the requested metric\n",
      "     |          An instance of the requested distance metric class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class PrecisionRecallDisplay(sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin)\n",
      "     |  PrecisionRecallDisplay(precision, recall, *, average_precision=None, estimator_name=None, pos_label=None, prevalence_pos_label=None)\n",
      "     |  \n",
      "     |  Precision Recall visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.PrecisionRecallDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  precision : ndarray\n",
      "     |      Precision values.\n",
      "     |  \n",
      "     |  recall : ndarray\n",
      "     |      Recall values.\n",
      "     |  \n",
      "     |  average_precision : float, default=None\n",
      "     |      Average precision. If None, the average precision is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, then the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The class considered as the positive class. If None, the class will not\n",
      "     |      be shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  prevalence_pos_label : float, default=None\n",
      "     |      The prevalence of the positive label. It is used for plotting the\n",
      "     |      chance level line. If None, the chance level line will not be plotted\n",
      "     |      even if `plot_chance_level` is set to True when plotting.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Precision recall curve.\n",
      "     |  \n",
      "     |  chance_level_ : matplotlib Artist or None\n",
      "     |      The chance level line. It is `None` if the chance level is not plotted.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with precision recall curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  precision_recall_curve : Compute precision-recall pairs for different\n",
      "     |      probability thresholds.\n",
      "     |  PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "     |      a binary classifier.\n",
      "     |  PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "     |      using predictions from a binary classifier.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The average precision (cf. :func:`~sklearn.metrics.average_precision_score`) in\n",
      "     |  scikit-learn is computed without any interpolation. To be consistent with\n",
      "     |  this metric, the precision-recall curve is plotted without any\n",
      "     |  interpolation as well (step-wise style).\n",
      "     |  \n",
      "     |  You can change this style by passing the keyword argument\n",
      "     |  `drawstyle=\"default\"` in :meth:`plot`, :meth:`from_estimator`, or\n",
      "     |  :meth:`from_predictions`. However, the curve will not be strictly\n",
      "     |  consistent with the reported average precision.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import (precision_recall_curve,\n",
      "     |  ...                              PrecisionRecallDisplay)\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
      "     |  >>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PrecisionRecallDisplay\n",
      "     |      sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision, recall, *, average_precision=None, estimator_name=None, pos_label=None, prevalence_pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, plot_chance_level=False, chance_level_kw=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : Matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of precision recall curve for labeling. If `None`, use\n",
      "     |          `estimator_name` if not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level. The chance level is the prevalence\n",
      "     |          of the positive label computed from the data passed during\n",
      "     |          :meth:`from_estimator` or :meth:`from_predictions` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision_score`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, pos_label=None, drop_intermediate=False, response_method='auto', name=None, ax=None, plot_chance_level=False, chance_level_kw=None, **kwargs) from builtins.type\n",
      "     |      Plot precision-recall curve given an estimator and some data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics. By default, `estimators.classes_[1]`\n",
      "     |          is considered as the positive class.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=False\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted precision-recall curve. This is useful in order to\n",
      "     |          create lighter precision-recall curves.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'},             default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, no name is used.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level. The chance level is the prevalence\n",
      "     |          of the positive label computed from the data passed during\n",
      "     |          :meth:`from_estimator` or :meth:`from_predictions` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_predictions : Plot precision-recall curve\n",
      "     |          using estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision_score`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> PrecisionRecallDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, pos_label=None, drop_intermediate=False, name=None, ax=None, plot_chance_level=False, chance_level_kw=None, **kwargs) from builtins.type\n",
      "     |      Plot precision-recall curve given binary class predictions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True binary labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=False\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted precision-recall curve. This is useful in order to\n",
      "     |          create lighter precision-recall curves.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level. The chance level is the prevalence\n",
      "     |          of the positive label computed from the data passed during\n",
      "     |          :meth:`from_estimator` or :meth:`from_predictions` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_estimator : Plot precision-recall curve\n",
      "     |          using an estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision_score`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> y_pred = clf.predict_proba(X_test)[:, 1]\n",
      "     |      >>> PrecisionRecallDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PredictionErrorDisplay(builtins.object)\n",
      "     |  PredictionErrorDisplay(*, y_true, y_pred)\n",
      "     |  \n",
      "     |  Visualization of the prediction error of a regression model.\n",
      "     |  \n",
      "     |  This tool can display \"residuals vs predicted\" or \"actual vs predicted\"\n",
      "     |  using scatter plots to qualitatively assess the behavior of a regressor,\n",
      "     |  preferably on held-out data points.\n",
      "     |  \n",
      "     |  See the details in the docstrings of\n",
      "     |  :func:`~sklearn.metrics.PredictionErrorDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PredictionErrorDisplay.from_predictions` to\n",
      "     |  create a visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, read\n",
      "     |  more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For details regarding interpreting these plots, refer to the\n",
      "     |  :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  y_true : ndarray of shape (n_samples,)\n",
      "     |      True values.\n",
      "     |  \n",
      "     |  y_pred : ndarray of shape (n_samples,)\n",
      "     |      Prediction values.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Optimal line representing `y_true == y_pred`. Therefore, it is a\n",
      "     |      diagonal line for `kind=\"predictions\"` and a horizontal line for\n",
      "     |      `kind=\"residuals\"`.\n",
      "     |  \n",
      "     |  errors_lines_ : matplotlib Artist or None\n",
      "     |      Residual lines. If `with_errors=False`, then it is set to `None`.\n",
      "     |  \n",
      "     |  scatter_ : matplotlib Artist\n",
      "     |      Scatter data points.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with the different matplotlib axis.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the scatter and lines.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PredictionErrorDisplay.from_estimator : Prediction error visualization\n",
      "     |      given an estimator and some data.\n",
      "     |  PredictionErrorDisplay.from_predictions : Prediction error visualization\n",
      "     |      given the true and predicted targets.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> ridge = Ridge().fit(X, y)\n",
      "     |  >>> y_pred = ridge.predict(X)\n",
      "     |  >>> display = PredictionErrorDisplay(y_true=y, y_pred=y_pred)\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, y_true, y_pred)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, kind='residual_vs_predicted', scatter_kwargs=None, line_kwargs=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e. difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |      \n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, kind='residual_vs_predicted', subsample=1000, random_state=None, ax=None, scatter_kwargs=None, line_kwargs=None) from builtins.type\n",
      "     |      Plot the prediction error given a regressor and some data.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      read more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For details regarding interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted regressor or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a regressor.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e. difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      subsample : float, int or None, default=1_000\n",
      "     |          Sampling the samples to be shown on the scatter plot. If `float`,\n",
      "     |          it should be between 0 and 1 and represents the proportion of the\n",
      "     |          original dataset. If `int`, it represents the number of samples\n",
      "     |          display on the scatter plot. If `None`, no subsampling will be\n",
      "     |          applied. by default, 1000 samples or less will be displayed.\n",
      "     |      \n",
      "     |      random_state : int or RandomState, default=None\n",
      "     |          Controls the randomness when `subsample` is not `None`.\n",
      "     |          See :term:`Glossary <random_state>` for details.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |          Object that stores the computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PredictionErrorDisplay : Prediction error visualization for regression.\n",
      "     |      PredictionErrorDisplay.from_predictions : Prediction error visualization\n",
      "     |          given the true and predicted targets.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_diabetes\n",
      "     |      >>> from sklearn.linear_model import Ridge\n",
      "     |      >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |      >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |      >>> ridge = Ridge().fit(X, y)\n",
      "     |      >>> disp = PredictionErrorDisplay.from_estimator(ridge, X, y)\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, kind='residual_vs_predicted', subsample=1000, random_state=None, ax=None, scatter_kwargs=None, line_kwargs=None) from builtins.type\n",
      "     |      Plot the prediction error given the true and predicted targets.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      read more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For details regarding interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True target values.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Predicted target values.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e. difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      subsample : float, int or None, default=1_000\n",
      "     |          Sampling the samples to be shown on the scatter plot. If `float`,\n",
      "     |          it should be between 0 and 1 and represents the proportion of the\n",
      "     |          original dataset. If `int`, it represents the number of samples\n",
      "     |          display on the scatter plot. If `None`, no subsampling will be\n",
      "     |          applied. by default, 1000 samples or less will be displayed.\n",
      "     |      \n",
      "     |      random_state : int or RandomState, default=None\n",
      "     |          Controls the randomness when `subsample` is not `None`.\n",
      "     |          See :term:`Glossary <random_state>` for details.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |          Object that stores the computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PredictionErrorDisplay : Prediction error visualization for regression.\n",
      "     |      PredictionErrorDisplay.from_estimator : Prediction error visualization\n",
      "     |          given an estimator and some data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_diabetes\n",
      "     |      >>> from sklearn.linear_model import Ridge\n",
      "     |      >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |      >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |      >>> ridge = Ridge().fit(X, y)\n",
      "     |      >>> y_pred = ridge.predict(X)\n",
      "     |      >>> disp = PredictionErrorDisplay.from_predictions(y_true=y, y_pred=y_pred)\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RocCurveDisplay(sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin)\n",
      "     |  RocCurveDisplay(*, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  ROC Curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.RocCurveDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  tpr : ndarray\n",
      "     |      True positive rate.\n",
      "     |  \n",
      "     |  roc_auc : float, default=None\n",
      "     |      Area under ROC curve. If None, the roc_auc score is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The class considered as the positive class when computing the roc auc\n",
      "     |      metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |      as the positive class.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      ROC Curve.\n",
      "     |  \n",
      "     |  chance_level_ : matplotlib Artist or None\n",
      "     |      The chance level line. It is `None` if the chance level is not plotted.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with ROC Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |  RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given an estimator and some data.\n",
      "     |  RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given the true and predicted values.\n",
      "     |  roc_auc_score : Compute the area under the ROC curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
      "     |  >>> roc_auc = metrics.auc(fpr, tpr)\n",
      "     |  >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
      "     |  ...                                   estimator_name='example estimator')\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RocCurveDisplay\n",
      "     |      sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, plot_chance_level=False, chance_level_kw=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', pos_label=None, name=None, ax=None, plot_chance_level=False, chance_level_kw=None, **kwargs) from builtins.type\n",
      "     |      Create a ROC Curve display from an estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the roc auc\n",
      "     |          metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |          as the positive class.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          The ROC Curve display.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_predictions : ROC Curve visualization given the\n",
      "     |          probabilities of scores of a classifier.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> RocCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, drop_intermediate=True, pos_label=None, name=None, ax=None, plot_chance_level=False, chance_level_kw=None, **kwargs) from builtins.type\n",
      "     |      Plot ROC curve given the true and predicted values.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by “decision_function” on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_estimator : ROC Curve visualization given an\n",
      "     |          estimator and some data.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> RocCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or int\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
      "            imbalanced datasets.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2.0\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (``label_true``)\n",
      "        with :math:`V` (``labels_pred``) will return the same score value. This can\n",
      "        be useful to measure the agreement of two independent label assignments\n",
      "        strategies on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'max' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative. The value is\n",
      "           in adjusted nats (based on the natural logarithm).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        mutual_info_score : Mutual Information (not adjusted for chance).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation). The adjusted Rand index is bounded below by -0.5 for\n",
      "        especially discordant clusterings.\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=int\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=int\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ARI : float\n",
      "           Similarity score between -0.5 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n",
      "          adjusted Rand index, Psychological Methods 2004\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        .. [Chacon] :doi:`Minimum adjusted Rand index for two clusterings of a given size,\n",
      "          2022, J. E. Chacón and A. I. Rastrojo <10.1007/s11634-022-00491-w>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        ARI may take a negative value for especially discordant labelings that\n",
      "        are a worse choice than the expected value of random labels::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 1, 0, 1])\n",
      "          -0.5\n",
      "    \n",
      "    auc(x, y)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array-like of shape (n,)\n",
      "            X coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : array-like of shape (n,)\n",
      "            Y coordinates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "            Area Under the Curve.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "    \n",
      "    average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores.\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by :term:`decision_function` on some classifiers).\n",
      "        \n",
      "        average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "            Average precision score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.83...\n",
      "        >>> y_true = np.array([0, 0, 1, 1, 2, 2])\n",
      "        >>> y_scores = np.array([\n",
      "        ...     [0.7, 0.2, 0.1],\n",
      "        ...     [0.4, 0.3, 0.3],\n",
      "        ...     [0.1, 0.8, 0.1],\n",
      "        ...     [0.2, 0.3, 0.5],\n",
      "        ...     [0.4, 0.4, 0.2],\n",
      "        ...     [0.1, 0.2, 0.7],\n",
      "        ... ])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.77...\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy.\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, while keeping perfect performance at a score\n",
      "            of 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "            Balanced accuracy score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Compute average precision (AP) from prediction\n",
      "            scores.\n",
      "        precision_score : Compute the precision score.\n",
      "        recall_score : Compute the recall score.\n",
      "        roc_auc_score : Compute Area Under the Receiver Operating Characteristic\n",
      "            Curve (ROC AUC) from prediction scores.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_proba=None, *, sample_weight=None, pos_label=None, y_prob='deprecated')\n",
      "        Compute the Brier score loss.\n",
      "        \n",
      "        The smaller the Brier score loss, the better, hence the naming with \"loss\".\n",
      "        The Brier score measures the mean squared difference between the predicted\n",
      "        probability and the actual outcome. The Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). It can be decomposed as the sum of refinement loss and\n",
      "        calibration loss.\n",
      "        \n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter `pos_label`, which defaults to\n",
      "        the greater label unless `y_true` is all 0 or all -1, in which case\n",
      "        `pos_label` defaults to 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <brier_score_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_proba : array-like of shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            Label of the positive class. `pos_label` will be inferred in the\n",
      "            following manner:\n",
      "        \n",
      "            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n",
      "            * else if `y_true` contains string, an error will be raised and\n",
      "              `pos_label` should be explicitly specified;\n",
      "            * otherwise, `pos_label` defaults to the greater label,\n",
      "              i.e. `np.unique(y_true)[-1]`.\n",
      "        \n",
      "        y_prob : array-like of shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "            .. deprecated:: 1.5\n",
      "                `y_prob` is deprecated and will be removed in 1.7. Use\n",
      "                `y_proba` instead.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score loss.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio of the sum of between-cluster dispersion and\n",
      "        of within-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> from sklearn.cluster import KMeans\n",
      "        >>> from sklearn.metrics import calinski_harabasz_score\n",
      "        >>> X, _ = make_blobs(random_state=0)\n",
      "        >>> kmeans = KMeans(n_clusters=3, random_state=0,).fit(X)\n",
      "        >>> calinski_harabasz_score(X, kmeans.labels_)\n",
      "        114.8...\n",
      "    \n",
      "    check_scoring(estimator=None, scoring=None, *, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit' or None, default=None\n",
      "            The object to use to fit the data. If `None`, then this function may error\n",
      "            depending on `allow_none`.\n",
      "        \n",
      "        scoring : str, callable, list, tuple, or dict, default=None\n",
      "            Scorer to use. If `scoring` represents a single score, one can use:\n",
      "        \n",
      "            - a single string (see :ref:`scoring_parameter`);\n",
      "            - a callable (see :ref:`scoring`) that returns a single value.\n",
      "        \n",
      "            If `scoring` represents multiple scores, one can use:\n",
      "        \n",
      "            - a list or tuple of unique strings;\n",
      "            - a callable returning a dictionary where the keys are the metric\n",
      "              names and the values are the metric scorers;\n",
      "            - a dictionary with metric names as keys and callables a values.\n",
      "        \n",
      "            If None, the provided estimator object's `score` method is used.\n",
      "        \n",
      "        allow_none : bool, default=False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> from sklearn.metrics import check_scoring\n",
      "        >>> from sklearn.tree import DecisionTreeClassifier\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)\n",
      "        >>> scorer = check_scoring(classifier, scoring='accuracy')\n",
      "        >>> scorer(classifier, X, y)\n",
      "        0.96...\n",
      "    \n",
      "    class_likelihood_ratios(y_true, y_pred, *, labels=None, sample_weight=None, raise_warning=True)\n",
      "        Compute binary classification positive and negative likelihood ratios.\n",
      "        \n",
      "        The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`\n",
      "        where the sensitivity or recall is the ratio `tp / (tp + fn)` and the\n",
      "        specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1\n",
      "        - sensitivity) / specificity`. Here `tp` is the number of true positives,\n",
      "        `fp` the number of false positives, `tn` is the number of true negatives and\n",
      "        `fn` the number of false negatives. Both class likelihood ratios can be used\n",
      "        to obtain post-test probabilities given a pre-test probability.\n",
      "        \n",
      "        `LR+` ranges from 1 to infinity. A `LR+` of 1 indicates that the probability\n",
      "        of predicting the positive class is the same for samples belonging to either\n",
      "        class; therefore, the test is useless. The greater `LR+` is, the more a\n",
      "        positive prediction is likely to be a true positive when compared with the\n",
      "        pre-test probability. A value of `LR+` lower than 1 is invalid as it would\n",
      "        indicate that the odds of a sample being a true positive decrease with\n",
      "        respect to the pre-test odds.\n",
      "        \n",
      "        `LR-` ranges from 0 to 1. The closer it is to 0, the lower the probability\n",
      "        of a given sample to be a false negative. A `LR-` of 1 means the test is\n",
      "        useless because the odds of having the condition did not change after the\n",
      "        test. A value of `LR-` greater than 1 invalidates the classifier as it\n",
      "        indicates an increase in the odds of a sample belonging to the positive\n",
      "        class after being classified as negative. This is the case when the\n",
      "        classifier systematically predicts the opposite of the true label.\n",
      "        \n",
      "        A typical application in medicine is to identify the positive/negative class\n",
      "        to the presence/absence of a disease, respectively; the classifier being a\n",
      "        diagnostic test; the pre-test probability of an individual having the\n",
      "        disease can be the prevalence of such disease (proportion of a particular\n",
      "        population found to be affected by a medical condition); and the post-test\n",
      "        probabilities would be the probability that the condition is truly present\n",
      "        given a positive test result.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <class_likelihood_ratios>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            List of labels to index the matrix. This may be used to select the\n",
      "            positive and negative classes with the ordering `labels=[negative_class,\n",
      "            positive_class]`. If `None` is given, those that appear at least once in\n",
      "            `y_true` or `y_pred` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        raise_warning : bool, default=True\n",
      "            Whether or not a case-specific warning message is raised when there is a\n",
      "            zero division. Even if the error is not raised, the function will return\n",
      "            nan in such cases.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        (positive_likelihood_ratio, negative_likelihood_ratio) : tuple\n",
      "            A tuple of two float, the first containing the Positive likelihood ratio\n",
      "            and the second the Negative likelihood ratio.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        When `false positive == 0`, the positive likelihood ratio is undefined.\n",
      "        When `true negative == 0`, the negative likelihood ratio is undefined.\n",
      "        When `true positive + false negative == 0` both ratios are undefined.\n",
      "        In such cases, `UserWarning` will be raised if raise_warning=True.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing\n",
      "               <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import class_likelihood_ratios\n",
      "        >>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])\n",
      "        (1.5, 0.75)\n",
      "        >>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
      "        >>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred)\n",
      "        (1.33..., 0.66...)\n",
      "        >>> y_true = np.array([\"non-zebra\", \"zebra\", \"non-zebra\", \"zebra\", \"non-zebra\"])\n",
      "        >>> y_pred = np.array([\"zebra\", \"zebra\", \"non-zebra\", \"non-zebra\", \"non-zebra\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred)\n",
      "        (1.5, 0.75)\n",
      "        \n",
      "        To avoid ambiguities, use the notation `labels=[negative_class,\n",
      "        positive_class]`\n",
      "        \n",
      "        >>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
      "        >>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred, labels=[\"non-cat\", \"cat\"])\n",
      "        (1.5, 0.75)\n",
      "    \n",
      "    classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "        Build a text report showing the main classification metrics.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_labels,), default=None\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : array-like of shape (n_labels,), default=None\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int, default=2\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool, default=False\n",
      "            If True, return output as dict.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : str or dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), and sample average (only for multilabel classification).\n",
      "            Micro average (averaging the total true positives, false negatives and\n",
      "            false positives) is only shown for multi-label or multi-class\n",
      "            with a subset of classes, because it corresponds to accuracy\n",
      "            otherwise and would be the same for all metrics.\n",
      "            See also :func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support: Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        confusion_matrix: Compute confusion matrix to evaluate the accuracy of a\n",
      "            classification.\n",
      "        multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
      "        Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array-like of shape (n_samples,)\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array-like of shape (n_samples,)\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If `None`, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : {'linear', 'quadratic'}, default=None\n",
      "            Weighting type to calculate the score. `None` means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               <10.1177/001316446002000104>`\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import cohen_kappa_score\n",
      "        >>> y1 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
      "        >>> y2 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"negative\"]\n",
      "        >>> cohen_kappa_score(y1, y2)\n",
      "        0.6875\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Compute completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` and\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If ``None`` is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (n_classes, n_classes)\n",
      "            Confusion matrix whose i-th row and j-th\n",
      "            column entry indicates the number of\n",
      "            samples with true label being i-th class\n",
      "            and predicted label being j-th class.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "            given an estimator, the data, and the label.\n",
      "        ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "            given the true and predicted labels.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc. as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, *, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : tuple (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : tuple (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : 'jaccard' or callable, default='jaccard'\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        consensus_score : float\n",
      "           Consensus score, a non-negative value, sum of similarities\n",
      "           divided by size of larger set.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import consensus_score\n",
      "        >>> a = ([[True, False], [False, True]], [[False, True], [True, False]])\n",
      "        >>> b = ([[False, True], [True, False]], [[True, False], [False, True]])\n",
      "        >>> consensus_score(a, b, similarity='jaccard')\n",
      "        1.0\n",
      "    \n",
      "    coverage_error(y_true, y_score, *, sample_weight=None)\n",
      "        Coverage error measure.\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "            The coverage error.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import coverage_error\n",
      "        >>> y_true = [[1, 0, 0], [0, 1, 1]]\n",
      "        >>> y_score = [[1, 0, 0], [0, 1, 1]]\n",
      "        >>> coverage_error(y_true, y_score)\n",
      "        1.5\n",
      "    \n",
      "    d2_absolute_error_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        :math:`D^2` regression score function, fraction of absolute error explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical median of `y_true`\n",
      "        as constant prediction, disregarding the input features,\n",
      "        gets a :math:`D^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The :math:`D^2` score with an absolute error deviance\n",
      "            or ndarray of scores if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Like :math:`R^2`, :math:`D^2` score may be negative\n",
      "        (it need not actually be the square of a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "         References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_absolute_error_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        0.764...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.691...\n",
      "        >>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.8125    , 0.57142857])\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        -1.0\n",
      "    \n",
      "    d2_log_loss_score(y_true, y_pred, *, sample_weight=None, labels=None)\n",
      "        :math:`D^2` score function, fraction of log loss explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always predicts the per-class proportions\n",
      "        of `y_true`, disregarding the input features, gets a D^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score_classification>`.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            The actuals labels for the n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`~sklearn.preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        d2 : float or ndarray of floats\n",
      "            The D^2 score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Like R^2, D^2 score may be negative (it need not actually be the square of\n",
      "        a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for a single sample and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "    \n",
      "    d2_pinball_score(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        :math:`D^2` regression score function, fraction of pinball loss explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical alpha-quantile of\n",
      "        `y_true` as constant prediction, disregarding the input features,\n",
      "        gets a :math:`D^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha : float, default=0.5\n",
      "            Slope of the pinball deviance. It determines the quantile level alpha\n",
      "            for which the pinball deviance and also D2 are optimal.\n",
      "            The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The :math:`D^2` score with a pinball deviance\n",
      "            or ndarray of scores if `multioutput='raw_values'`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Like :math:`R^2`, :math:`D^2` score may be negative\n",
      "        (it need not actually be the square of a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for a single point and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "         References\n",
      "        ----------\n",
      "        .. [1] Eq. (7) of `Koenker, Roger; Machado, José A. F. (1999).\n",
      "               \"Goodness of Fit and Related Inference Processes for Quantile Regression\"\n",
      "               <https://doi.org/10.1080/01621459.1999.10473882>`_\n",
      "        .. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_pinball_score\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 3, 3]\n",
      "        >>> d2_pinball_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> d2_pinball_score(y_true, y_pred, alpha=0.9)\n",
      "        0.772...\n",
      "        >>> d2_pinball_score(y_true, y_pred, alpha=0.1)\n",
      "        -1.045...\n",
      "        >>> d2_pinball_score(y_true, y_true, alpha=0.1)\n",
      "        1.0\n",
      "    \n",
      "    d2_tweedie_score(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        :math:`D^2` regression score function, fraction of Tweedie deviance explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical mean of `y_true` as\n",
      "        constant prediction, disregarding the input features, gets a D^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to r2_score.\n",
      "              y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The D^2 score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Like R^2, D^2 score may be negative (it need not actually be the square of\n",
      "        a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_tweedie_score\n",
      "        >>> y_true = [0.5, 1, 2.5, 7]\n",
      "        >>> y_pred = [1, 1, 5, 3.5]\n",
      "        >>> d2_tweedie_score(y_true, y_pred)\n",
      "        0.285...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=1)\n",
      "        0.487...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=2)\n",
      "        0.630...\n",
      "        >>> d2_tweedie_score(y_true, y_true, power=2)\n",
      "        1.0\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Compute the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import davies_bouldin_score\n",
      "        >>> X = [[0, 1], [1, 1], [3, 4]]\n",
      "        >>> labels = [0, 0, 1]\n",
      "        >>> davies_bouldin_score(X, labels)\n",
      "        0.12...\n",
      "    \n",
      "    dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)\n",
      "        Compute Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\n",
      "        ndcg_score) is preferred.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        log_base : float, default=2\n",
      "            Base of the logarithm used for the discount. A low value means a\n",
      "            sharper discount (top results are more important).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        discounted_cumulative_gain : float\n",
      "            The averaged sample DCG scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n",
      "            Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n",
      "            have a score between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013).\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import dcg_score\n",
      "        >>> # we have ground-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict scores for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> dcg_score(true_relevance, scores)\n",
      "        9.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute\n",
      "        >>> dcg_score(true_relevance, scores, k=2)\n",
      "        5.63...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average true\n",
      "        >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
      "        >>> dcg_score(true_relevance, scores, k=1)\n",
      "        7.5\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> dcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        5.0\n",
      "    \n",
      "    det_curve(y_true, y_score, pos_label=None, sample_weight=None)\n",
      "        Compute error rates for different probability thresholds.\n",
      "        \n",
      "        .. note::\n",
      "           This metric is used for evaluation of ranking and error tradeoffs of\n",
      "           a binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <det_curve>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape of (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (n_thresholds,)\n",
      "            False positive rate (FPR) such that element i is the false positive\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false acceptance probability or fall-out.\n",
      "        \n",
      "        fnr : ndarray of shape (n_thresholds,)\n",
      "            False negative rate (FNR) such that element i is the false negative\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false rejection or miss rate.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing score values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "            some data.\n",
      "        DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "            predicted labels.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        precision_recall_curve : Compute precision-recall curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import det_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n",
      "        >>> fpr\n",
      "        array([0.5, 0.5, 0. ])\n",
      "        >>> fnr\n",
      "        array([0. , 0.5, 0.5])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Compute the distance matrix between each pair from a vector array X and Y.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve a better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as `np.float32`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\n",
      "        Explained variance regression score function.\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        In the particular case when ``y_true`` is constant, the explained variance\n",
      "        score is not finite: it is either ``NaN`` (perfect predictions) or\n",
      "        ``-Inf`` (imperfect predictions). To prevent such non-finite numbers to\n",
      "        pollute higher-level experiments such as a grid search cross-validation,\n",
      "        by default these cases are replaced with 1.0 (perfect predictions) or 0.0\n",
      "        (imperfect predictions) respectively. If ``force_finite``\n",
      "        is set to ``False``, this score falls back on the original :math:`R^2`\n",
      "        definition.\n",
      "        \n",
      "        .. note::\n",
      "           The Explained Variance score is similar to the\n",
      "           :func:`R^2 score <r2_score>`, with the notable difference that it\n",
      "           does not account for systematic offsets in the prediction. Most often\n",
      "           the :func:`R^2 score <r2_score>` should be preferred.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        force_finite : bool, default=True\n",
      "            Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
      "            data should be replaced with real numbers (``1.0`` if prediction is\n",
      "            perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
      "            for hyperparameters' search procedures (e.g. grid search\n",
      "            cross-validation).\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        r2_score :\n",
      "            Similar metric, but accounting for systematic offsets in\n",
      "            prediction.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.983...\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> explained_variance_score(y_true, y_pred, force_finite=False)\n",
      "        nan\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2 + 1e-8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> explained_variance_score(y_true, y_pred, force_finite=False)\n",
      "        -inf\n",
      "    \n",
      "    f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "        \n",
      "        The F1 score can be interpreted as a harmonic mean of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}}\n",
      "        \n",
      "        Where :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the\n",
      "        number of false negatives, and :math:`\\text{FP}` is the number of false positives.\n",
      "        F1 is by default\n",
      "        calculated as 0.0 when there are no true positives, false negatives, or\n",
      "        false positives.\n",
      "        \n",
      "        Support beyond :term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and F1 score for both classes are computed, then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        F1 score for all `labels` are either returned or averaged depending on the\n",
      "        `average` parameter. Use `labels` specify the set of labels to calculate F1 score\n",
      "        for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative.\n",
      "        \n",
      "            Notes:\n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fbeta_score : Compute the F-beta score.\n",
      "        precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "            and support.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive + false negative == 0`` (i.e. a class\n",
      "        is completely absent from both ``y_true`` or ``y_pred``), f-score is\n",
      "        undefined. In such cases, by default f-score will be set to 0.0, and\n",
      "        ``UndefinedMetricWarning`` will be raised. This behavior can be modified by\n",
      "        setting the ``zero_division`` parameter.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        \n",
      "        >>> # binary classification\n",
      "        >>> y_true_empty = [0, 0, 0, 0, 0, 0]\n",
      "        >>> y_pred_empty = [0, 0, 0, 0, 0, 0]\n",
      "        >>> f1_score(y_true_empty, y_pred_empty)\n",
      "        0.0...\n",
      "        >>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)\n",
      "        1.0...\n",
      "        >>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)\n",
      "        nan...\n",
      "        \n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.66666667, 1.        , 0.66666667])\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F-beta score.\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter represents the ratio of recall importance to\n",
      "        precision importance. `beta > 1` gives more weight to recall, while\n",
      "        `beta < 1` favors precision. For example, `beta = 2` makes recall twice\n",
      "        as important as precision, while `beta = 0.5` does the opposite.\n",
      "        Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0`\n",
      "        only precision.\n",
      "        \n",
      "        The formula for F-beta score is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}\n",
      "                            {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}\n",
      "        \n",
      "        Where :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the\n",
      "        number of false positives, and :math:`\\text{fn}` is the number of false negatives.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is\n",
      "        ignored and F-beta score for both classes are computed, then averaged or both\n",
      "        returned (when `average=None`). Similarly, for :term:`multiclass` and\n",
      "        :term:`multilabel` targets, F-beta score for all `labels` are either returned or\n",
      "        averaged depending on the `average` parameter. Use `labels` specify the set of\n",
      "        labels to calculate F-beta score for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Determines the weight of recall in the combined score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative.\n",
      "        \n",
      "            Notes:\n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "            and support.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive + false negative == 0``, f-score\n",
      "        returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified by setting ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "        >>> y_pred_empty = [0, 0, 0, 0, 0, 0]\n",
      "        >>> fbeta_score(y_true, y_pred_empty,\n",
      "        ...             average=\"macro\", zero_division=np.nan, beta=0.5)\n",
      "        0.12...\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, *, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e. the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=int\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=int\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_parameter>`.\n",
      "        :func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names\n",
      "        of all available scorers.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str, callable or None\n",
      "            Scoring method as string. If callable it is returned as is.\n",
      "            If None, returns None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When passed a string, this function always returns a copy of the scorer\n",
      "        object. Calling `get_scorer` twice for the same scorer results in two\n",
      "        separate scorer objects.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.dummy import DummyClassifier\n",
      "        >>> from sklearn.metrics import get_scorer\n",
      "        >>> X = np.reshape([0, 1, -1, -0.5, 2], (-1, 1))\n",
      "        >>> y = np.array([0, 1, 1, 0, 1])\n",
      "        >>> classifier = DummyClassifier(strategy=\"constant\", constant=0).fit(X, y)\n",
      "        >>> accuracy = get_scorer(\"accuracy\")\n",
      "        >>> accuracy(classifier, X, y)\n",
      "        0.4\n",
      "    \n",
      "    get_scorer_names()\n",
      "        Get the names of all available scorers.\n",
      "        \n",
      "        These names can be passed to :func:`~sklearn.metrics.get_scorer` to\n",
      "        retrieve the scorer object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of str\n",
      "            Names of all available scorers.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import get_scorer_names\n",
      "        >>> all_scorers = get_scorer_names()\n",
      "        >>> type(all_scorers)\n",
      "        <class 'list'>\n",
      "        >>> all_scorers[:3]\n",
      "        ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score']\n",
      "        >>> \"roc_auc\" in all_scorers\n",
      "        True\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized).\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Average hinge loss.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292.\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero\n",
      "               <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(random_state=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC()\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels=labels)\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "            Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n",
      "        \n",
      "        completeness : float\n",
      "            Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        v_measure : float\n",
      "            Harmonic mean of the first two.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import homogeneity_completeness_v_measure\n",
      "        >>> y_true, y_pred = [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 2, 2]\n",
      "        >>> homogeneity_completeness_v_measure(y_true, y_pred)\n",
      "        (0.71..., 0.77..., 0.73...)\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Jaccard similarity coefficient score.\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return the\n",
      "        Jaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`,\n",
      "        `pos_label` is ignored and scores for both classes are computed, then averaged or\n",
      "        both returned (when `average=None`). Similarly, for :term:`multiclass` and\n",
      "        :term:`multilabel` targets, scores for all `labels` are either returned or\n",
      "        averaged depending on the `average` parameter. Use `labels` specify the set of\n",
      "        labels to calculate the score for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when there\n",
      "            there are no negative values in predictions and labels. If set to\n",
      "            \"warn\", this acts like 0, but a warning is also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of shape (n_unique_labels,), dtype=np.float64\n",
      "            The Jaccard score. When `average` is not `None`, a single scalar is\n",
      "            returned.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Function for calculating the accuracy score.\n",
      "        f1_score : Function for calculating the F1 score.\n",
      "        multilabel_confusion_matrix : Function for computing a confusion matrix                                  for each class or sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])\n",
      "        0.6666...\n",
      "        \n",
      "        In the 2D comparison case (e.g. image similarity):\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average=\"micro\")\n",
      "        0.6\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)\n",
      "        Compute ranking-based average precision.\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Ranking-based average precision score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, *, sample_weight=None)\n",
      "        Compute Ranking loss measure.\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Average number of label pairs that are incorrectly ordered given\n",
      "            y_score weighted by the size of the label set and the number of labels not\n",
      "            in the label set.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import label_ranking_loss\n",
      "        >>> y_true = [[1, 0, 0], [0, 0, 1]]\n",
      "        >>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]\n",
      "        >>> label_ranking_loss(y_true, y_score)\n",
      "        0.75...\n",
      "    \n",
      "    log_loss(y_true, y_pred, *, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
      "        for its training data ``y_true``.\n",
      "        The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
      "        a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
      "        loss is:\n",
      "        \n",
      "        .. math::\n",
      "            L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`~sklearn.preprocessing.LabelBinarizer`.\n",
      "        \n",
      "            `y_pred` values are clipped to `[eps, 1-eps]` where `eps` is the machine\n",
      "            precision for `y_pred`'s dtype.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "    \n",
      "    make_scorer(score_func, *, response_method=None, greater_is_better=True, needs_proba='deprecated', needs_threshold='deprecated', **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        A scorer is a wrapper around an arbitrary metric or loss function that is called\n",
      "        with the signature `scorer(estimator, X, y_true, **kwargs)`.\n",
      "        \n",
      "        It is accepted in all scikit-learn estimators or functions allowing a `scoring`\n",
      "        parameter.\n",
      "        \n",
      "        The parameter `response_method` allows to specify which method of the estimator\n",
      "        should be used to feed the scoring/loss function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        response_method : {\"predict_proba\", \"decision_function\", \"predict\"} or             list/tuple of such str, default=None\n",
      "        \n",
      "            Specifies the response method to use get prediction from an estimator\n",
      "            (i.e. :term:`predict_proba`, :term:`decision_function` or\n",
      "            :term:`predict`). Possible choices are:\n",
      "        \n",
      "            - if `str`, it corresponds to the name to the method to return;\n",
      "            - if a list or tuple of `str`, it provides the method names in order of\n",
      "              preference. The method returned corresponds to the first method in\n",
      "              the list and which is implemented by `estimator`.\n",
      "            - if `None`, it is equivalent to `\"predict\"`.\n",
      "        \n",
      "            .. versionadded:: 1.4\n",
      "        \n",
      "        greater_is_better : bool, default=True\n",
      "            Whether `score_func` is a score function (default), meaning high is\n",
      "            good, or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the `score_func`.\n",
      "        \n",
      "        needs_proba : bool, default=False\n",
      "            Whether `score_func` requires `predict_proba` to get probability\n",
      "            estimates out of a classifier.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class, shape\n",
      "            `(n_samples,)`).\n",
      "        \n",
      "            .. deprecated:: 1.4\n",
      "               `needs_proba` is deprecated in version 1.4 and will be removed in\n",
      "               1.6. Use `response_method=\"predict_proba\"` instead.\n",
      "        \n",
      "        needs_threshold : bool, default=False\n",
      "            Whether `score_func` takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a `decision_function` or `predict_proba` method.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class or the decision\n",
      "            function, shape `(n_samples,)`).\n",
      "        \n",
      "            For example `average_precision` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "            .. deprecated:: 1.4\n",
      "               `needs_threshold` is deprecated in version 1.4 and will be removed\n",
      "               in 1.6. Use `response_method=(\"decision_function\", \"predict_proba\")`\n",
      "               instead to preserve the same behaviour.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to `score_func`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, response_method='predict', beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC).\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview.\n",
      "           <10.1093/bioinformatics/16.5.412>`\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient (phi coefficient)\n",
      "           <https://en.wikipedia.org/wiki/Phi_coefficient>`_.\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        The max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85...\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute percentage error (MAPE) regression loss.\n",
      "        \n",
      "        Note here that the output is not a percentage in the range [0, 100]\n",
      "        and a value of 100 does not mean 100% but 1e2. Furthermore, the output\n",
      "        can be arbitrarily high when `y_true` is small (which is specific to the\n",
      "        metric) or when `abs(y_true - y_pred)` is large (which is common for most\n",
      "        regression metrics). Read more in the\n",
      "        :ref:`User Guide <mean_absolute_percentage_error>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "            If input is list then the shape must be (n_outputs,).\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute percentage error\n",
      "            is returned for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAPE output is non-negative floating point. The best value is 0.0.\n",
      "            But note that bad predictions can lead to arbitrarily large\n",
      "            MAPE values, especially if some `y_true` values are very close to zero.\n",
      "            Note that we return a large value instead of `inf` when `y_true` is zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_percentage_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.3273...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.5515...\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.6198...\n",
      "        >>> # the value when some element of the y_true is zero is arbitrarily high because\n",
      "        >>> # of the division by epsilon\n",
      "        >>> y_true = [1., 0., 2.4, 7.]\n",
      "        >>> y_pred = [1.2, 0.1, 2.4, 8.]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        112589990684262.48\n",
      "    \n",
      "    mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Gamma deviance regression loss.\n",
      "        \n",
      "        Gamma deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=2`. It is invariant to scaling of\n",
      "        the target variable, and measures relative errors.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true > 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_gamma_deviance\n",
      "        >>> y_true = [2, 0.5, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_gamma_deviance(y_true, y_pred)\n",
      "        1.0568...\n",
      "    \n",
      "    mean_pinball_loss(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        Pinball loss for quantile regression.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pinball_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha : float, slope of the pinball loss, default=0.5,\n",
      "            This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,\n",
      "            `alpha=0.95` is minimized by estimators of the 95th percentile.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            The pinball loss output is a non-negative floating point. The best\n",
      "            value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_pinball_loss\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.1)\n",
      "        0.0\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.9)\n",
      "        0.0\n",
      "    \n",
      "    mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Poisson deviance regression loss.\n",
      "        \n",
      "        Poisson deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=1`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true >= 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_poisson_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_poisson_deviance(y_true, y_pred)\n",
      "        1.4260...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared='deprecated')\n",
      "        Mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        squared : bool, default=True\n",
      "            If True returns MSE value, if False returns RMSE value.\n",
      "        \n",
      "            .. deprecated:: 1.4\n",
      "               `squared` is deprecated in 1.4 and will be removed in 1.6.\n",
      "               Use :func:`~sklearn.metrics.root_mean_squared_error`\n",
      "               instead to calculate the root mean squared error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared='deprecated')\n",
      "        Mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        squared : bool, default=True\n",
      "            If True returns MSLE (mean squared log error) value.\n",
      "            If False returns RMSLE (root mean squared log error) value.\n",
      "        \n",
      "            .. deprecated:: 1.4\n",
      "               `squared` is deprecated in 1.4 and will be removed in 1.6.\n",
      "               Use :func:`~sklearn.metrics.root_mean_squared_log_error`\n",
      "               instead to calculate the root mean squared logarithmic error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.060...\n",
      "    \n",
      "    mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        Mean Tweedie deviance regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to\n",
      "              mean_squared_error. y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_tweedie_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_tweedie_deviance(y_true, y_pred, power=1)\n",
      "        1.4260...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\n",
      "        Median absolute error regression loss.\n",
      "        \n",
      "        Median absolute error output is non-negative floating point. The best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values. Array-like value defines\n",
      "            weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data).\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : ndarray of shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute confusion matrix to evaluate the accuracy of a\n",
      "            classifier.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `multilabel_confusion_matrix` calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while\n",
      "        :func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix\n",
      "        for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, *, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels\n",
      "        of the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (i.e\n",
      "        ``label_true``) with :math:`V` (i.e. ``label_pred``) will return the\n",
      "        same score value. This can be useful to measure the agreement of two\n",
      "        independent label assignments strategies on the same dataset when the\n",
      "        real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        contingency : {array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n",
      "            A contingency matrix given by the\n",
      "            :func:`~sklearn.metrics.cluster.contingency_matrix` function. If value\n",
      "            is ``None``, it will be computed, otherwise the given value is used,\n",
      "            with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value, measured in nats using the\n",
      "           natural logarithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted against chance Mutual Information.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mutual_info_score\n",
      "        >>> labels_true = [0, 1, 1, 0, 1, 0]\n",
      "        >>> labels_pred = [0, 1, 0, 0, 1, 1]\n",
      "        >>> mutual_info_score(labels_true, labels_pred)\n",
      "        0.056...\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features), default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan, float or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)\n",
      "        Compute Normalized Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount. Then divide by the best possible\n",
      "        score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
      "        0 and 1.\n",
      "        \n",
      "        This ranking metric returns a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked. Negative values in `y_true` may result in an output\n",
      "            that is not between 0 and 1.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If `None`, use all\n",
      "            outputs.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        normalized_discounted_cumulative_gain : float in [0., 1.]\n",
      "            The averaged NDCG scores for all samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dcg_score : Discounted Cumulative Gain (not normalized).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013)\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import ndcg_score\n",
      "        >>> # we have ground-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict some scores (relevance) for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.69...\n",
      "        >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute.\n",
      "        >>> ndcg_score(true_relevance, scores, k=4)\n",
      "        0.35...\n",
      "        >>> # the normalization takes k into account so a perfect answer\n",
      "        >>> # would still get 1.0\n",
      "        >>> ndcg_score(true_relevance, true_relevance, k=4)\n",
      "        1.0...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average (normalized)\n",
      "        >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n",
      "        >>> ndcg_score(true_relevance, scores, k=1)\n",
      "        0.75...\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> ndcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        0.5...\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'geometric' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           Score between 0.0 and 1.0 in normalized nats (based on the natural\n",
      "           logarithm). 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n",
      "            against chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pair_confusion_matrix(labels_true, labels_pred)\n",
      "        Pair confusion matrix arising from two clusterings [1]_.\n",
      "        \n",
      "        The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
      "        between two clusterings by considering all pairs of samples and counting\n",
      "        pairs that are assigned into the same or into different clusters under\n",
      "        the true and predicted clusterings.\n",
      "        \n",
      "        Considering a pair of samples that is clustered together a positive pair,\n",
      "        then as in binary classification the count of true negatives is\n",
      "        :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
      "        :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (2, 2), dtype=np.int64\n",
      "            The contingency matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.rand_score : Rand Score.\n",
      "        sklearn.metrics.adjusted_rand_score : Adjusted Rand Score.\n",
      "        sklearn.metrics.adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n",
      "               Journal of Classification 2, 193–218 (1985).\n",
      "               <10.1007/BF01908075>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have all non-zero entries on the\n",
      "        diagonal regardless of actual label values:\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
      "          >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          array([[8, 0],\n",
      "                 [0, 4]]...\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may be not always pure, hence penalized, and\n",
      "        have some off-diagonal non-zero entries:\n",
      "        \n",
      "          >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          array([[8, 2],\n",
      "                 [0, 2]]...\n",
      "        \n",
      "        Note that the matrix is not symmetric.\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix.\n",
      "        If the input is a vector array, the distances are computed.\n",
      "        If the input is a distances matrix, it is returned instead.\n",
      "        If the input is a collection of non-numeric data (e.g. a list of strings or a\n",
      "        boolean array), a custom metric must be passed.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        .. note::\n",
      "            `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n",
      "        \n",
      "        .. note::\n",
      "            `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "            The \"euclidean\" and \"cosine\" metrics rely heavily on BLAS which is already\n",
      "            multithreaded. So, increasing `n_jobs` would likely cause oversubscription\n",
      "            and quickly degrade performance.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        sklearn.metrics.pairwise.paired_distances : Computes the distances between\n",
      "            corresponding elements of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_distances\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> pairwise_distances(X, Y, metric='sqeuclidean')\n",
      "        array([[1., 2.],\n",
      "               [2., 1.]])\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "            .. note::\n",
      "               `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n",
      "        \n",
      "            .. note::\n",
      "               `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Distances between every pair of samples of X and Y.\n",
      "        pairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\n",
      "            returns the distances.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> pairwise_distances_argmin(X, Y)\n",
      "        array([0, 1])\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "            .. note::\n",
      "               `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n",
      "        \n",
      "            .. note::\n",
      "               `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            The array of minimum distances. `distances[i]` is the distance between\n",
      "            the i-th row in X and the argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Distances between every pair of samples of X and Y.\n",
      "        pairwise_distances_argmin : Same as `pairwise_distances_argmin_min` but only\n",
      "            returns the argmins.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> argmin, distances = pairwise_distances_argmin_min(X, Y)\n",
      "        >>> argmin\n",
      "        array([0, 1])\n",
      "        >>> distances\n",
      "        array([1., 1.])\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be\n",
      "        stored at once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is\n",
      "        run on each chunk and its return values are concatenated into lists,\n",
      "        arrays or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects.\n",
      "            Returning None is useful for in-place operations, rather than\n",
      "            reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter,\n",
      "            or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on\n",
      "            each pair of instances (rows) and the resulting value recorded.\n",
      "            The callable should take two arrays from X as input and return a\n",
      "            value indicating the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by\n",
      "            breaking down the pairwise matrix into n_jobs even slices and\n",
      "            computing them in parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : float, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_kernels\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> pairwise_kernels(X, Y, metric='linear')\n",
      "        array([[0., 0.],\n",
      "               [1., 2.]])\n",
      "    \n",
      "    precision_recall_curve(y_true, y_score=None, *, pos_label=None, sample_weight=None, drop_intermediate=False, probas_pred='deprecated')\n",
      "        Compute precision-recall pairs for different probability thresholds.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold. This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        The first precision and recall values are precision=class balance and recall=1.0\n",
      "        which corresponds to a classifier that always predicts the positive class.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, or non-thresholded measure of decisions (as returned by\n",
      "            `decision_function` on some classifiers).\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=False\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted precision-recall curve. This is useful in order to create\n",
      "            lighter precision-recall curves.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        probas_pred : array-like of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, or non-thresholded measure of decisions (as returned by\n",
      "            `decision_function` on some classifiers).\n",
      "        \n",
      "            .. deprecated:: 1.5\n",
      "                `probas_pred` is deprecated and will be removed in 1.7. Use\n",
      "                `y_score` instead.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_thresholds + 1,)\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : ndarray of shape (n_thresholds + 1,)\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall where `n_thresholds = len(np.unique(probas_pred))`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "            a binary classifier.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "            using predictions from a binary classifier.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision\n",
      "        array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.1 , 0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
      "        Compute precision, recall, F-measure and support for each class.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label a negative sample as\n",
      "        positive.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and metrics for both classes are computed, then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        metrics for all `labels` are either returned or averaged depending on the `average`\n",
      "        parameter. Use `labels` specify the set of labels to calculate metrics for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'binary', 'micro', 'macro', 'samples', 'weighted'},             default=None\n",
      "            If ``None``, the metrics for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : list, tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division:\n",
      "               - recall: when there are no positive labels\n",
      "               - precision: when there are no positive predictions\n",
      "               - f-score: both\n",
      "        \n",
      "            Notes:\n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision score.\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall score.\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score.\n",
      "        \n",
      "        support : None (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined. When\n",
      "        ``true positive + false negative + false positive == 0``, f-score is\n",
      "        undefined. In such cases, by default the metric will be set to 0, and\n",
      "        ``UndefinedMetricWarning`` will be raised. This behavior can be modified\n",
      "        with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the precision.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and precision for both classes are computed, then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        precision for all `labels` are either returned or averaged depending on the\n",
      "        `average` parameter. Use `labels` specify the set of labels to calculate precision\n",
      "        for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division.\n",
      "        \n",
      "            Notes:\n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "            number of true positives and ``fn`` the number of false negatives.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.33..., 0.        , 0.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.33..., 1.        , 1.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)\n",
      "        array([0.33...,        nan,        nan])\n",
      "        \n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 1. , 1. ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\n",
      "        :math:`R^2` (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). In the general case when the true y is\n",
      "        non-constant, a constant model that always predicts the average y\n",
      "        disregarding the input features would get a :math:`R^2` score of 0.0.\n",
      "        \n",
      "        In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
      "        is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
      "        (imperfect predictions). To prevent such non-finite numbers to pollute\n",
      "        higher-level experiments such as a grid search cross-validation, by default\n",
      "        these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
      "        predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
      "        prevent this fix from happening.\n",
      "        \n",
      "        Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
      "        is identical to the\n",
      "        :func:`Explained Variance score <explained_variance_score>`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        force_finite : bool, default=True\n",
      "            Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
      "            data should be replaced with real numbers (``1.0`` if prediction is\n",
      "            perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
      "            for hyperparameters' search procedures (e.g. grid search\n",
      "            cross-validation).\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
      "        actually be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted')\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> r2_score(y_true, y_pred, force_finite=False)\n",
      "        nan\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2 + 1e-8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> r2_score(y_true, y_pred, force_finite=False)\n",
      "        -inf\n",
      "    \n",
      "    rand_score(labels_true, labels_pred)\n",
      "        Rand index.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings [1]_ [2]_.\n",
      "        \n",
      "        The raw RI score [3]_ is:\n",
      "        \n",
      "            RI = (number of agreeing pairs) / (number of pairs)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        RI : float\n",
      "           Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n",
      "           perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Score.\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n",
      "           Journal of Classification 2, 193–218 (1985).\n",
      "           <10.1007/BF01908075>`.\n",
      "        \n",
      "        .. [2] `Wikipedia: Simple Matching Coefficient\n",
      "            <https://en.wikipedia.org/wiki/Simple_matching_coefficient>`_\n",
      "        \n",
      "        .. [3] `Wikipedia: Rand Index <https://en.wikipedia.org/wiki/Rand_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import rand_score\n",
      "          >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized:\n",
      "        \n",
      "          >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.83...\n",
      "    \n",
      "    recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the recall.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        recall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and recall for both classes are computed then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        recall for all `labels` are either returned or averaged depending on the `average`\n",
      "        parameter. Use `labels` specify the set of labels to calculate recall for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall. Weighted recall\n",
      "                is equal to accuracy.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division.\n",
      "        \n",
      "            Notes:\n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n",
      "            number of true positives and ``fp`` the number of false positives.\n",
      "        balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n",
      "            datasets.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "        ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0. , 0. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.5, 1. , 1. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=np.nan)\n",
      "        array([0.5, nan, nan])\n",
      "        \n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1. , 1. , 0.5])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\n",
      "        \n",
      "        Note: this implementation can be used with binary, multiclass and\n",
      "        multilabel classification, but some restrictions apply (see Parameters).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True labels or binary label indicators. The binary and multiclass cases\n",
      "            expect labels with shape (n_samples,) while the multilabel case expects\n",
      "            binary label indicators with shape (n_samples, n_classes).\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores.\n",
      "        \n",
      "            * In the binary case, it corresponds to an array of shape\n",
      "              `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "              decision values can be provided. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label**,\n",
      "              i.e. `estimator.classes_[1]` and thus\n",
      "              `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "              corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "              See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "            * In the multiclass case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)` of probability estimates provided by the\n",
      "              `predict_proba` method. The probability estimates **must**\n",
      "              sum to 1 across the possible classes. In addition, the order of the\n",
      "              class scores must correspond to the order of ``labels``,\n",
      "              if provided, or else to the numerical or lexicographical order of\n",
      "              the labels in ``y_true``. See more information in the\n",
      "              :ref:`User guide <roc_auc_multiclass>`;\n",
      "            * In the multilabel case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "              `predict_proba` method and the non-thresholded decision values by\n",
      "              the `decision_function` method. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label for each\n",
      "              output** of the classifier. See more information in the\n",
      "              :ref:`User guide <roc_auc_multilabel>`.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned.\n",
      "            Otherwise, this determines the type of averaging performed on the data.\n",
      "            Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "            'weighted' averages. For multiclass targets, `average=None` is only\n",
      "            implemented for `multi_class='ovr'` and `average='micro'` is only\n",
      "            implemented for `multi_class='ovr'`.\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, default=None\n",
      "            If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "            should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "            computation currently is not supported for multiclass.\n",
      "        \n",
      "        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "            Only used for multiclass targets. Determines the type of configuration\n",
      "            to use. The default value raises an error, so either\n",
      "            ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "        \n",
      "            ``'ovr'``:\n",
      "                Stands for One-vs-rest. Computes the AUC of each class\n",
      "                against the rest [3]_ [4]_. This\n",
      "                treats the multiclass case in the same way as the multilabel case.\n",
      "                Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "                because class imbalance affects the composition of each of the\n",
      "                'rest' groupings.\n",
      "            ``'ovo'``:\n",
      "                Stands for One-vs-one. Computes the average AUC of all\n",
      "                possible pairwise combinations of classes [5]_.\n",
      "                Insensitive to class imbalance when\n",
      "                ``average == 'macro'``.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Only used for multiclass targets. List of labels that index the\n",
      "            classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "            order of the labels in ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "            Area Under the Curve score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Gini Coefficient is a summary measure of the ranking ability of binary\n",
      "        classifiers. It is expressed using the area under of the ROC as follows:\n",
      "        \n",
      "        G = 2 * AUC - 1\n",
      "        \n",
      "        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation\n",
      "        will ensure that random guessing will yield a score of 0 in expectation, and it is\n",
      "        upper bounded by 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "               probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "               #IS-00-04, Stern School of Business, New York University.\n",
      "        \n",
      "        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "                Recognition Letters, 27(8), 861-874.\n",
      "                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "        \n",
      "        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "                Under the ROC Curve for Multiple Class Classification Problems.\n",
      "                Machine Learning, 45(2), 171-186.\n",
      "                <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "        .. [6] `Wikipedia entry for the Gini coefficient\n",
      "                <https://en.wikipedia.org/wiki/Gini_coefficient>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Binary case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "        0.99...\n",
      "        >>> roc_auc_score(y, clf.decision_function(X))\n",
      "        0.99...\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "        0.99...\n",
      "        \n",
      "        Multilabel case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.datasets import make_multilabel_classification\n",
      "        >>> from sklearn.multioutput import MultiOutputClassifier\n",
      "        >>> X, y = make_multilabel_classification(random_state=0)\n",
      "        >>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      "        >>> # get a list of n_output containing probability arrays of shape\n",
      "        >>> # (n_samples, n_classes)\n",
      "        >>> y_pred = clf.predict_proba(X)\n",
      "        >>> # extract the positive columns for each output\n",
      "        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n",
      "        >>> roc_auc_score(y, y_pred, average=None)\n",
      "        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n",
      "        >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "        >>> clf = RidgeClassifierCV().fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n",
      "    \n",
      "    roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC).\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (>2,)\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        tpr : ndarray of shape (>2,)\n",
      "            Increasing true positive rates such that element `i` is the true\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `np.inf`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        An arbitrary threshold is added for the case `tpr=0` and `fpr=0` to\n",
      "        ensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n",
      "        `np.inf`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    root_mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Root mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import root_mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> root_mean_squared_error(y_true, y_pred)\n",
      "        0.612...\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> root_mean_squared_error(y_true, y_pred)\n",
      "        0.822...\n",
      "    \n",
      "    root_mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Root mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import root_mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> root_mean_squared_log_error(y_true, y_pred)\n",
      "        0.199...\n",
      "    \n",
      "    silhouette_samples(X, labels, *, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 ``<= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array. If\n",
      "            a sparse matrix is provided, CSR format should be favoured avoiding\n",
      "            an additional copy.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Label values for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`~sklearn.metrics.pairwise_distances`.\n",
      "            If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n",
      "            Precomputed distance matrices must have 0 along the diagonal.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array-like of shape (n_samples,)\n",
      "            Silhouette Coefficients for each sample.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import silhouette_samples\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> from sklearn.cluster import KMeans\n",
      "        >>> X, y = make_blobs(n_samples=50, random_state=42)\n",
      "        >>> kmeans = KMeans(n_clusters=3, random_state=42)\n",
      "        >>> labels = kmeans.fit_predict(X)\n",
      "        >>> silhouette_samples(X, labels)\n",
      "        array([...])\n",
      "    \n",
      "    silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is ``2 <= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`~sklearn.metrics.pairwise_distances`. If ``X`` is\n",
      "            the distance array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int, default=None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for selecting a subset of samples.\n",
      "            Used when ``sample_size is not None``.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> from sklearn.cluster import KMeans\n",
      "        >>> from sklearn.metrics import silhouette_score\n",
      "        >>> X, y = make_blobs(random_state=42)\n",
      "        >>> kmeans = KMeans(n_clusters=2, random_state=42)\n",
      "        >>> silhouette_score(X, kmeans.fit_predict(X))\n",
      "        0.49...\n",
      "    \n",
      "    top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)\n",
      "        Top-k Accuracy classification score.\n",
      "        \n",
      "        This metric computes the number of times where the correct label is among\n",
      "        the top `k` labels predicted (ranked by predicted scores). Note that the\n",
      "        multilabel case isn't covered here.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <top_k_accuracy_score>`\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True labels.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores. These can be either probability estimates or\n",
      "            non-thresholded decision values (as returned by\n",
      "            :term:`decision_function` on some classifiers).\n",
      "            The binary case expects scores with shape (n_samples,) while the\n",
      "            multiclass case expects scores with shape (n_samples, n_classes).\n",
      "            In the multiclass case, the order of the class scores must\n",
      "            correspond to the order of ``labels``, if provided, or else to\n",
      "            the numerical or lexicographical order of the labels in ``y_true``.\n",
      "            If ``y_true`` does not contain all the labels, ``labels`` must be\n",
      "            provided.\n",
      "        \n",
      "        k : int, default=2\n",
      "            Number of most likely outcomes considered to find the correct label.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If `True`, return the fraction of correctly classified samples.\n",
      "            Otherwise, return the number of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Multiclass only. List of labels that index the classes in ``y_score``.\n",
      "            If ``None``, the numerical or lexicographical order of the labels in\n",
      "            ``y_true`` is used. If ``y_true`` does not contain all the labels,\n",
      "            ``labels`` must be provided.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The top-k accuracy score. The best performance is 1 with\n",
      "            `normalize == True` and the number of samples with\n",
      "            `normalize == False`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In cases where two or more labels are assigned equal predicted scores,\n",
      "        the labels with the highest indices will be chosen first. This might\n",
      "        impact the result if the correct label falls after the threshold because\n",
      "        of that.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import top_k_accuracy_score\n",
      "        >>> y_true = np.array([0, 1, 2, 2])\n",
      "        >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
      "        ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n",
      "        ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n",
      "        ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2)\n",
      "        0.75\n",
      "        >>> # Not normalizing gives the number of \"correctly\" classified samples\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
      "        3\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, *, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harm completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1.0\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\lemon\\anaconda3\\envs\\big\\lib\\site-packages\\sklearn\\metrics\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대부분의 지표는 sklearn.metric에 있다\n",
    "import sklearn.metrics\n",
    "help(sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# 정확도\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 숫자형 평가\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(acc)\n",
    "\n",
    "# 범주형 평가\n",
    "acc_str = accuracy_score(y_true_str, y_pred_str)\n",
    "print(acc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# 정밀도\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 숫자형 평가\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(precision)\n",
    "\n",
    "# 문자형 평가\n",
    "# 어떻값을 1(양성)으로 평가할지 명시\n",
    "precision_str = precision_score(y_true_str, y_pred_str, pos_label='A')\n",
    "print(precision_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# 재현율\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# 숫자형 평가\n",
    "recall= recall_score(y_true, y_pred)\n",
    "print(recall)\n",
    "\n",
    "# 범주형 평가\n",
    "recall_str = recall_score(y_true_str, y_pred_str, pos_label='A')\n",
    "print(recall_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# f1 스코어\n",
    "# 불균형 데이터에\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f1)\n",
    "\n",
    "f1_str = f1_score(y_true_str, y_pred_str, pos_label='A')\n",
    "print(f1_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_AUC\n",
    "# 의 경우 예측 확률을 넣어야해서 predict_proba()를 이용\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
